{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using MINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MNIST(root = 'DATA/', download=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset contains \\\n",
    "60,000 training images = _train = True_ \\\n",
    "10,000 testing images = _train = False_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data =  60000\n",
      "Testing Data =  10000\n"
     ]
    }
   ],
   "source": [
    "train_data = MNIST( 'DATA/', train = True)\n",
    "print('Training Data = ', len(train_data))\n",
    "\n",
    "test_data = MNIST( 'DATA/', train = False)\n",
    "print('Testing Data = ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_data[0]\n",
    "ax = plt.imshow(img, cmap = 'gray')\n",
    "ax.set_label('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc4a9c77fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we'll use the ToTensor transform to convert images into PyTorch tensors.\n",
    "\n",
    "train_data = MNIST(root='Data/', train=True, transform=transforms.ToTensor())\n",
    "\n",
    "img, label = train_data[0]\n",
    "plt.subplot(121)\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "\n",
    "print(torch.max(img), torch.min(img))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img[0,10:15,10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader - Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make Data set Ready\n",
    "2. Generate Model (Forward pass)\n",
    "3. Calculate Loss and Optimize (Backpass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset= test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "$(pred = x @ w^T + b)$\n",
    "\n",
    "From 60,000 training set:\\\n",
    "10,000 --> for **Validation Data Set**\\\n",
    "50,000 --> for **Training Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set = random_split(train_data, [50000, 10000])\n",
    "\n",
    "print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 1x28x28 image tensor needs to be flattened out into a vector of size 784 (28*28), before being passed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights torch.Size([10, 784])\n",
      "bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "input_size = 28*28\n",
    "n_classes = 10\n",
    "\n",
    "model = nn.Linear(input_size, n_classes)\n",
    "\n",
    "print('Weights', model.weight.shape)\n",
    "print('bias', model.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "# for img, label in train_loader:\n",
    "#     print(img)\n",
    "#     print(img.shape)\n",
    "#     print(label)\n",
    "    \n",
    "#     # it gives an error of size missmatch: [2800 x 28]\n",
    "#     output=model(img) \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Derived Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert (1x28x28) into 784 size vector\n",
    "# We need to Flatten '.reshape'\n",
    "\n",
    "# Our own defined function:\n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, n_classes)\n",
    "    \n",
    "    def fwd(self, img_flat):\n",
    "        img_flat = img_flat.reshape(-1, 784)\n",
    "        # -1 -> consider the first dimension from (100,1,28,28)\n",
    "        return self.linear(img_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights torch.Size([10, 784])\n",
      "bias torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-2.1386e-02, -6.4187e-03,  2.1061e-02,  ..., -3.7937e-03,\n",
       "          -2.5735e-02, -2.0696e-03],\n",
       "         [ 1.3357e-02,  1.4827e-02, -2.1932e-02,  ...,  7.8543e-03,\n",
       "           2.7900e-02,  2.9369e-02],\n",
       "         [-2.5810e-02, -5.3822e-03,  2.3208e-02,  ..., -3.5403e-02,\n",
       "          -1.0144e-02,  3.4455e-02],\n",
       "         ...,\n",
       "         [ 1.9012e-02, -5.6665e-03, -5.6077e-03,  ...,  3.6664e-05,\n",
       "           1.9219e-03, -6.3595e-03],\n",
       "         [ 3.5403e-02, -2.1747e-02,  2.5800e-02,  ...,  1.4759e-03,\n",
       "          -2.2265e-02, -3.4509e-02],\n",
       "         [ 1.2166e-02,  1.4513e-02, -3.3587e-02,  ...,  5.2020e-03,\n",
       "          -9.5686e-03, -3.2238e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0106,  0.0101, -0.0162, -0.0230,  0.0342, -0.0257,  0.0328,  0.0179,\n",
       "          0.0107, -0.0051], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Weights', model.weight.shape)\n",
    "print('bias', model.bias.shape)\n",
    "\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n",
      "Output Shape torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "# Use the MODEL\n",
    "model = MnistModel()\n",
    "\n",
    "# Check the data\n",
    "for img, label in train_loader:\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    # it gives an error of size missmatch: [2800 x 28]\n",
    "    output=model.fwd(img) \n",
    "    break\n",
    "    \n",
    "print('Output Shape', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2181, -0.0679, -0.0743,  0.0104, -0.0962,  0.0497,  0.1026, -0.1985,\n",
       "        -0.2884, -0.0262], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative outputs makes no sense\n",
    "Pass them through **SoftMax** function\\\n",
    "It brings all the values from 0-1\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8041, 0.9344, 0.9284, 1.0105, 0.9083, 1.0510, 1.1081, 0.8199, 0.7494,\n",
       "        0.9741], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = torch.exp(output[0])\n",
    "numerator\n",
    "\n",
    "# Negative values are gone\n",
    "# but still not between 0 - 1\n",
    "# as Probability adds to = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([0.0866, 0.1006, 0.1000, 0.1088, 0.0978, 0.1132, 0.1193, 0.0883, 0.0807,\n",
      "        0.1049], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# SoftMax Equation\n",
    "\n",
    "probability = numerator/torch.sum(numerator)\n",
    "print(probability.shape)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch SoftMax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0866, 0.1006, 0.1000, 0.1088, 0.0978, 0.1132, 0.1193, 0.0883, 0.0807,\n",
      "        0.1049], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# size = (batch_size, 1, 28,28) => torch.Size([100, 1, 28, 28])\n",
    "probability = F.softmax(output, dim=1)  # as dim=0 are the dim of batch_size, We want to apply across 10 digits\n",
    "\n",
    "print(probability[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we pick the Highest Probability value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1193, 0.1356, 0.1320, 0.1333, 0.1385, 0.1424, 0.1356, 0.1312, 0.1451,\n",
      "        0.1217, 0.1181, 0.1250, 0.1289, 0.1355, 0.1484, 0.1338, 0.1548, 0.1381,\n",
      "        0.1422, 0.1705, 0.1263, 0.1512, 0.1115, 0.1489, 0.1450, 0.1390, 0.1378,\n",
      "        0.1496, 0.1428, 0.1595, 0.1278, 0.1427, 0.1268, 0.1416, 0.1295, 0.1485,\n",
      "        0.1262, 0.1236, 0.1425, 0.1472, 0.1329, 0.1602, 0.1448, 0.1310, 0.1328,\n",
      "        0.1194, 0.1326, 0.1177, 0.1387, 0.1203, 0.1518, 0.1611, 0.1413, 0.1268,\n",
      "        0.1326, 0.1390, 0.1302, 0.1258, 0.1221, 0.1326, 0.1279, 0.1300, 0.1291,\n",
      "        0.1319, 0.1282, 0.1803, 0.1276, 0.1702, 0.1481, 0.1623, 0.1136, 0.1218,\n",
      "        0.1300, 0.1255, 0.1265, 0.1529, 0.1174, 0.1655, 0.1281, 0.1264, 0.1539,\n",
      "        0.1265, 0.1582, 0.1341, 0.1342, 0.2024, 0.1096, 0.1213, 0.1245, 0.1763,\n",
      "        0.1428, 0.1215, 0.1266, 0.1307, 0.1245, 0.1311, 0.1210, 0.1418, 0.1227,\n",
      "        0.1568], grad_fn=<MaxBackward0>)\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "max_prob, max_pos = torch.max(probability, dim=1)\n",
    "\n",
    "print(max_prob)\n",
    "print(max_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 3, 9, 6, 6, 6, 2, 2, 8, 3, 3, 8, 6, 2, 6, 6, 2, 6, 6, 0, 6, 6, 6,\n",
      "        9, 6, 9, 6, 2, 2, 3, 6, 2, 6, 6, 6, 9, 9, 6, 6, 6, 6, 9, 3, 6, 6, 9, 6,\n",
      "        6, 9, 6, 6, 6, 6, 9, 6, 2, 9, 8, 9, 9, 9, 6, 6, 9, 6, 2, 6, 6, 6, 2, 3,\n",
      "        3, 9, 2, 6, 3, 6, 6, 9, 6, 8, 6, 9, 9, 6, 9, 8, 7, 6, 9, 2, 7, 9, 9, 6,\n",
      "        8, 6, 5, 6])\n",
      "tensor([5, 4, 9, 2, 0, 9, 2, 7, 7, 1, 7, 1, 1, 4, 7, 2, 3, 9, 3, 6, 7, 1, 5, 0,\n",
      "        9, 9, 5, 7, 9, 2, 7, 8, 8, 3, 5, 2, 0, 5, 0, 3, 5, 2, 4, 4, 5, 8, 4, 0,\n",
      "        1, 1, 7, 0, 0, 9, 1, 1, 5, 6, 1, 9, 1, 8, 6, 3, 5, 8, 9, 6, 2, 6, 7, 5,\n",
      "        7, 1, 9, 4, 1, 7, 9, 1, 6, 1, 2, 1, 3, 6, 5, 1, 5, 8, 8, 4, 0, 1, 8, 7,\n",
      "        3, 9, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "print(max_pos)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    accu = torch.tensor(torch.sum(prediction == label).item()/len(prediction))\n",
    "    # correct prediction/ total predictions = accuracy\n",
    "    print(\"Accuracy = \", accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  tensor(0.0900)\n"
     ]
    }
   ],
   "source": [
    "accuracy(output, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems:\n",
    "Cannot be used as a **Loss Function**\n",
    "1. == is not differentiable\n",
    "2. does not take consideration of actual probabilities - it does not look for prob for true value\n",
    "\n",
    "### Cross-Entropy\n",
    "Take the logarithm of probability\\\n",
    "Log(high prob) = small -ve value\\\n",
    "Log(low prob) = large -ve value\\\n",
    "Function is differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3502, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_cross = F.cross_entropy(output, label)\n",
    "print(loss_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

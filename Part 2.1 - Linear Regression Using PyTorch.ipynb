{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for determining the Crop yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = temp, rainfall and humidiy\n",
    "# output = apple, oranges\n",
    "\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# convert numpy data to Torch\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Apple = W_11 * temp + W_12*Rainfall + W_13*Humidity + bias_1$ \\\n",
    "$ Oranges = W_21 * temp + W_22*Rainfall + W_23*Humidity + bias_2$ \\\n",
    "\n",
    "weights = 3 X 2 \\ \n",
    "data = 3 X 2 \\\n",
    "therefore Matrix multiplication will be:\\\n",
    "                    $ X_{3X2} * W_{2X3}^T + b_2 $ \\\n",
    "We are assigning random weights and biases initally, then it will get adjusted depending on the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights =  tensor([[ 0.1147, -0.9016, -0.6377],\n",
      "        [ 0.4821, -0.3799, -1.5286]], requires_grad=True)\n",
      "bias =  tensor([ 1.0425, -0.5771], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define Weights\n",
    "\n",
    "w = torch.randn([2,3], requires_grad = True)\n",
    "b = torch.randn([2], requires_grad = True)\n",
    "\n",
    "print('weights = ', w)\n",
    "print('bias = ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MODEL\n",
    "\n",
    "def Reg_model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "# '@' represents Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -78.4165,  -56.5676],\n",
      "        [-108.6782,  -87.9682],\n",
      "        [-146.7853,  -98.1996],\n",
      "        [ -49.6263,  -24.2991],\n",
      "        [-122.2398, -110.7842]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "Element wise distance\n",
      "tensor([[-134.4165, -126.5676],\n",
      "        [-189.6782, -188.9682],\n",
      "        [-265.7853, -231.1996],\n",
      "        [ -71.6263,  -61.2991],\n",
      "        [-225.2398, -229.7841]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Predict the Model\n",
    "\n",
    "pred = Reg_model(inputs)\n",
    "print(pred)\n",
    "\n",
    "print('')\n",
    "print(targets)\n",
    "print('Element wise distance')\n",
    "print(pred - targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18067.8047, 16019.3623],\n",
      "        [35977.8047, 35708.9805],\n",
      "        [70641.8125, 53453.2617],\n",
      "        [ 5130.3223,  3757.5776],\n",
      "        [50732.9844, 52800.7539]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Negative is not good, convert it into positive\n",
    "\n",
    "print((pred-targets)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34229.0703, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# converting the whole data into single value\n",
    "# this single value represents the total deviation\n",
    "# Now, we have to reduce this number as minimus as possible\n",
    "\n",
    "# converting into single number\n",
    "diff = (pred-targets)\n",
    "print(torch.sum((diff)**2)/diff.numel())\n",
    "\n",
    "# .numel() => returns the length of matrix or vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "it is used to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss\n",
    "def msee(pred,targets):\n",
    "    diff = pred - targets\n",
    "    return torch.sum(diff**2)/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34229.0703, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = msee(pred, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "In Pytorch we can auto compute derative w.r.t weights and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss is the function of weights and biases\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1147, -0.9016, -0.6377],\n",
      "        [ 0.4821, -0.3799, -1.5286]], requires_grad=True)\n",
      "tensor([[-14608.7734, -17203.1543, -10350.3643],\n",
      "        [-13731.5049, -16157.0244,  -9859.7812]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find the lowest Loss - else, if some loss is there, then because of the square term, the loss boosts \\\n",
    "\\\n",
    "**Negative** Gradient(Derivative):\\\n",
    "* increasing value decreases the Loss\n",
    "* decreasing value increase the Loss \\\n",
    "\\\n",
    "**Positive** Gradient (Derivative):\\\n",
    "* increasing value increase the Loss\n",
    "* decreasing value decrease the Loss \\\n",
    "\\\n",
    "**Goal**\\\n",
    "To decrease the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-14608.7734, -17203.1543, -10350.3643],\n",
      "        [-13731.5049, -16157.0244,  -9859.7812]])\n",
      "tensor([-177.3492, -167.5637])\n"
     ]
    }
   ],
   "source": [
    "# running  y.backward() repeatedly, Just take repeated Derivative\n",
    "# therefore we have to clear the Gradient \n",
    "# everytime we go for calculation of new gradient\n",
    "\n",
    "# w.grad.zero_()\n",
    "# b.grad.zero_()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust weight and biases using **Gradient Descent**\n",
    "1. Generate Predictions \n",
    "2. Calcualte the Loss\n",
    "3. Compute Gradients w.r.t. weights and bias\n",
    "4. Adjust the weights by subtracting gradients\n",
    "5. reset the gradient to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -78.4165,  -56.5676],\n",
      "        [-108.6782,  -87.9682],\n",
      "        [-146.7853,  -98.1996],\n",
      "        [ -49.6263,  -24.2991],\n",
      "        [-122.2398, -110.7842]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "\n",
    "prediction = Reg_model(inputs)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34229.0703, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = msee(prediction, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-29217.5469, -34406.3086, -20700.7285],\n",
      "        [-27463.0098, -32314.0488, -19719.5625]])\n",
      "tensor([-354.6985, -335.1275])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use **torch.no_grad** to indicate to PyTorch that we shouldn't track, calculate or modify gradients while updating the weights and biases.\n",
    "* We multiply the gradients with a really small number (10^-5 in this case) as learning rate.\n",
    "* After we have updated the weights, we reset the gradients back to zero, to avoid affecting any future computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4068, -0.5576, -0.4307],\n",
      "        [ 0.7567, -0.0567, -1.3314]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Adjust the weights by subtracting small quantity of gradients\n",
    "# Adjust and Reset Gradients\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= (w.grad * learning_rate)\n",
    "    b -= (b.grad * learning_rate)\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w -= 0.01 * w.grad is an in-place operation, so it performs \n",
    "# calculation on existing w and updates the value.\n",
    "\n",
    "# However, w = w - 0.01 * w.grad is not in-place operation, \n",
    "# so it creates a new variable w, which does not have requires_grad set and so the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4068, -0.5576, -0.4307],\n",
      "        [ 0.7567, -0.0567, -1.3314]], requires_grad=True)\n",
      "tensor([ 1.0460, -0.5738], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15121.5234, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate LOSS\n",
    "\n",
    "prediction = Reg_model(inputs)\n",
    "loss = msee(prediction, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Multiple Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights =  tensor([[ 0.0624,  0.0322,  1.9052],\n",
      "        [-0.4808, -0.4674, -0.9244]], requires_grad=True)\n",
      "bias =  tensor([-0.4053,  0.3311], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn([2,3], requires_grad = True)\n",
    "b = torch.randn([2], requires_grad = True)\n",
    "\n",
    "print('weights = ', w)\n",
    "print('bias = ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27123.3008, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = Reg_model(inputs)\n",
    "loss = msee(prediction, targets)\n",
    "loss.backward()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "lr = 1e-5\n",
    "\n",
    "for i in range(epoch):\n",
    "    prediction = Reg_model(inputs)\n",
    "    loss = msee(prediction, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= (w.grad*lr)\n",
    "        b -= (b.grad*lr)\n",
    "        # b = b - learning_rate * eps  *** gives error\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(235.4134, grad_fn=<DivBackward0>)\n",
      "tensor([[-0.3422,  0.1560,  1.7287],\n",
      "        [ 0.2579,  0.8902, -0.0959]], requires_grad=True)\n",
      "tensor([-0.4096,  0.3420], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 59.4145,  74.7133],\n",
       "        [ 92.8368,  96.0235],\n",
       "        [ 90.8999, 136.4471],\n",
       "        [ 35.4715,  61.5129],\n",
       "        [111.9346,  96.8257]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Part 3.1 BRING ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "img_size = 28*28\n",
    "n_classes = 10\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data =  60000\n",
      "Testing Data =  10000\n"
     ]
    }
   ],
   "source": [
    "train_data = MNIST( root='~/Developer/PyTorch_YouTube/DATA/', train = True, transform = transforms.ToTensor() )\n",
    "print('Training Data = ', len(train_data))\n",
    "\n",
    "test_data = MNIST( root ='~/Developer/PyTorch_YouTube/DATA/', train = False, transform = transforms.ToTensor())\n",
    "print('Testing Data = ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_batch = DataLoader(dataset= test_data, batch_size=batch_size)\n",
    "\n",
    "# can be used only in for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set = random_split(train_data, [50000, 10000])\n",
    "\n",
    "print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    for batch in test_set:\n",
    "         Generate predictions\n",
    "         Calculate loss\n",
    "         Compute gradients\n",
    "         Update weights\n",
    "         Reset gradients\n",
    "    \n",
    "    # Validation phase\n",
    "    for batch in val_set:\n",
    "         Generate predictions\n",
    "         Calculate loss\n",
    "         Calculate metrics (accuracy etc.)\n",
    "     Calculate average validation loss & metrics\n",
    "    \n",
    "     Log epoch, loss & metrics for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Regression, Opti and Loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(img_size, n_classes)\n",
    "\n",
    "cross_loss = nn.CrossEntropyLoss() # computes SoftMax internally\n",
    "\n",
    "opti = torch.optim.SGD(model.parameters(), lr = lr)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Step 200/600, Loss: 2.2898\n",
      "Epoch 1/25, Step 400/600, Loss: 2.3046\n",
      "Epoch 1/25, Step 600/600, Loss: 2.2556\n",
      "Epoch 2/25, Step 200/600, Loss: 2.2438\n",
      "Epoch 2/25, Step 400/600, Loss: 2.2380\n",
      "Epoch 2/25, Step 600/600, Loss: 2.2224\n",
      "Epoch 3/25, Step 200/600, Loss: 2.2241\n",
      "Epoch 3/25, Step 400/600, Loss: 2.2000\n",
      "Epoch 3/25, Step 600/600, Loss: 2.1700\n",
      "Epoch 4/25, Step 200/600, Loss: 2.1295\n",
      "Epoch 4/25, Step 400/600, Loss: 2.1067\n",
      "Epoch 4/25, Step 600/600, Loss: 2.0643\n",
      "Epoch 5/25, Step 200/600, Loss: 2.0733\n",
      "Epoch 5/25, Step 400/600, Loss: 2.0824\n",
      "Epoch 5/25, Step 600/600, Loss: 2.0489\n",
      "Epoch 6/25, Step 200/600, Loss: 2.0256\n",
      "Epoch 6/25, Step 400/600, Loss: 2.0036\n",
      "Epoch 6/25, Step 600/600, Loss: 1.9760\n",
      "Epoch 7/25, Step 200/600, Loss: 1.9550\n",
      "Epoch 7/25, Step 400/600, Loss: 1.9208\n",
      "Epoch 7/25, Step 600/600, Loss: 1.9552\n",
      "Epoch 8/25, Step 200/600, Loss: 1.9400\n",
      "Epoch 8/25, Step 400/600, Loss: 1.9484\n",
      "Epoch 8/25, Step 600/600, Loss: 1.9094\n",
      "Epoch 9/25, Step 200/600, Loss: 1.8413\n",
      "Epoch 9/25, Step 400/600, Loss: 1.8821\n",
      "Epoch 9/25, Step 600/600, Loss: 1.9050\n",
      "Epoch 10/25, Step 200/600, Loss: 1.8430\n",
      "Epoch 10/25, Step 400/600, Loss: 1.8658\n",
      "Epoch 10/25, Step 600/600, Loss: 1.8286\n",
      "Epoch 11/25, Step 200/600, Loss: 1.7834\n",
      "Epoch 11/25, Step 400/600, Loss: 1.7693\n",
      "Epoch 11/25, Step 600/600, Loss: 1.7626\n",
      "Epoch 12/25, Step 200/600, Loss: 1.7026\n",
      "Epoch 12/25, Step 400/600, Loss: 1.7563\n",
      "Epoch 12/25, Step 600/600, Loss: 1.7657\n",
      "Epoch 13/25, Step 200/600, Loss: 1.6957\n",
      "Epoch 13/25, Step 400/600, Loss: 1.7217\n",
      "Epoch 13/25, Step 600/600, Loss: 1.7054\n",
      "Epoch 14/25, Step 200/600, Loss: 1.6601\n",
      "Epoch 14/25, Step 400/600, Loss: 1.6858\n",
      "Epoch 14/25, Step 600/600, Loss: 1.6416\n",
      "Epoch 15/25, Step 200/600, Loss: 1.6631\n",
      "Epoch 15/25, Step 400/600, Loss: 1.6613\n",
      "Epoch 15/25, Step 600/600, Loss: 1.6014\n",
      "Epoch 16/25, Step 200/600, Loss: 1.5612\n",
      "Epoch 16/25, Step 400/600, Loss: 1.6269\n",
      "Epoch 16/25, Step 600/600, Loss: 1.6134\n",
      "Epoch 17/25, Step 200/600, Loss: 1.5580\n",
      "Epoch 17/25, Step 400/600, Loss: 1.6073\n",
      "Epoch 17/25, Step 600/600, Loss: 1.5898\n",
      "Epoch 18/25, Step 200/600, Loss: 1.6288\n",
      "Epoch 18/25, Step 400/600, Loss: 1.5179\n",
      "Epoch 18/25, Step 600/600, Loss: 1.5203\n",
      "Epoch 19/25, Step 200/600, Loss: 1.5252\n",
      "Epoch 19/25, Step 400/600, Loss: 1.5169\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 25\n",
    "\n",
    "total_step = len(train_batch)  # 60,000 data size & 100 batches\n",
    "\n",
    "# with torch.no_grad():    \n",
    "for epoch in range(n_epoch):\n",
    "    for i, (image, label) in enumerate(train_batch):\n",
    "\n",
    "        # Flatten the image\n",
    "        img = image.reshape(-1, img_size)\n",
    "\n",
    "        # Forward Pass\n",
    "        output = model(img)\n",
    "        loss = cross_loss(output, label)\n",
    "\n",
    "        # Backward Pass\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        opti.step() # performs a parameter update w -= w.grad*lr\n",
    "\n",
    "        if (i+1)%200 ==0:\n",
    "            print('Epoch {}/{}, Step {}/{}, Loss: {:.4f}'\n",
    "                 .format(epoch+1, n_epoch, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_batch: # test loader can be used in a for loop\n",
    "        img = images.reshape(-1, img_size)\n",
    "        \n",
    "        output = model(img)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)  # check maximum along rows\n",
    "        \n",
    "        correct += (predicted == labels).sum()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    print('Accuracy = {} %'.format(100*correct//total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Individual Images\n",
    "\n",
    "At beginning we converted dataset compatiable to PyTorch = transforms.ToTensor()\\\n",
    "Therefore we have to undo the effect = use **transforms.ToPILImage()(img)**\\\n",
    "We will use **TEST Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_and_train(img, model):\n",
    "    img = img.reshape(-1, img.shape[1]*img.shape[2])\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    _, guess = torch.max(output.data, 1)\n",
    "    \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img, label = test_data[991]\n",
    "\n",
    "plt.imshow(transforms.ToPILImage()(img), cmap = 'gray')\n",
    "prediction = flat_and_train(img, model)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = range(100)\n",
    "fig, axs = plt.subplots(2,5, figsize=(12, 6), facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "\n",
    "    choice = np.random.choice(range(1000))\n",
    "    img, label = test_data[choice]\n",
    "\n",
    "    prediction = flat_and_train(img, model)\n",
    "\n",
    "    axs[i].imshow(transforms.ToPILImage()(img), cmap = 'gray')\n",
    "    axs[i].set_xlabel(str(prediction))\n",
    "    axs[i].set_title(str(label))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
